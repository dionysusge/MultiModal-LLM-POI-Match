"""
äº¤äº’ç‰¹å¾èåˆMLPä¼˜åŒ–æ–¹æ¡ˆ add gat
Author: Dionysus

ä¼˜åŒ–ç­–ç•¥ï¼š
1. é™ç»´å¤„ç†ï¼šå¯¹äº¤äº’ç‰¹å¾è¿›è¡Œé™ç»´ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©
2. æ‰¹å½’ä¸€åŒ–ï¼šæ·»åŠ BatchNormç¨³å®šè®­ç»ƒ
3. æ®‹å·®è¿æ¥ï¼šå¢åŠ æ®‹å·®è¿æ¥æå‡æ¢¯åº¦æµ
4. å­¦ä¹ ç‡è°ƒåº¦ï¼šä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡
5. ç‰¹å¾é€‰æ‹©ï¼šåªä¿ç•™æœ€æœ‰æ•ˆçš„äº¤äº’ç‰¹å¾
"""

import pandas as pd
import numpy as np
import pickle
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score
import argparse
import warnings
from tqdm import tqdm
warnings.filterwarnings("ignore")


class OptimizedInteractionExtractor(nn.Module):
    """ä¼˜åŒ–çš„äº¤äº’ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„äº¤äº’ç‰¹å¾æå–å™¨
        """
        super(OptimizedInteractionExtractor, self).__init__()
        
    def forward(self, feat_a, feat_b):
        """
        è®¡ç®—ä¸¤ä¸ªç‰¹å¾å‘é‡çš„ä¼˜åŒ–äº¤äº’ç‰¹å¾
        
        Args:
            feat_a: ç‰¹å¾å‘é‡A
            feat_b: ç‰¹å¾å‘é‡B
            
        Returns:
            äº¤äº’ç‰¹å¾å¼ é‡
        """
        # 1. å…ƒç´ çº§å·®å€¼ï¼ˆç»å¯¹å€¼ï¼‰
        diff = torch.abs(feat_a - feat_b)
        
        # 2. å…ƒç´ çº§ä¹˜ç§¯
        product = feat_a * feat_b
        
        # 3. ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ ‡é‡ï¼‰
        cos_sim = F.cosine_similarity(feat_a, feat_b, dim=1, eps=1e-8).unsqueeze(1)
        
        # 4. L2è·ç¦»ï¼ˆæ ‡é‡ï¼‰
        l2_dist = torch.norm(feat_a - feat_b, p=2, dim=1).unsqueeze(1)
        
        # 5. ç‰¹å¾å‡å€¼å·®å¼‚ï¼ˆæ ‡é‡ï¼‰
        mean_diff = torch.abs(torch.mean(feat_a, dim=1) - torch.mean(feat_b, dim=1)).unsqueeze(1)
        
        # 6. ç‰¹å¾æ–¹å·®æ¯”ï¼ˆæ ‡é‡ï¼‰
        var_a = torch.var(feat_a, dim=1).unsqueeze(1) + 1e-8
        var_b = torch.var(feat_b, dim=1).unsqueeze(1) + 1e-8
        var_ratio = torch.min(var_a, var_b) / torch.max(var_a, var_b)
        
        # æ‹¼æ¥äº¤äº’ç‰¹å¾ï¼šå·®å€¼ + ä¹˜ç§¯ + 4ä¸ªæ ‡é‡ç‰¹å¾
        interaction_features = torch.cat([
            diff, product, cos_sim, l2_dist, mean_diff, var_ratio
        ], dim=1)
        
        return interaction_features


class ResidualBlock(nn.Module):
    """æ®‹å·®å—"""
    
    def __init__(self, input_dim, hidden_dim, dropout=0.2):
        super(ResidualBlock, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, input_dim)
        self.bn2 = nn.BatchNorm1d(input_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.fc1(x)))
        out = self.dropout(out)
        out = self.bn2(self.fc2(out))
        out += residual  # æ®‹å·®è¿æ¥
        return F.relu(out)


class OptimizedFusionMLP(nn.Module):
    """ä¼˜åŒ–çš„äº¤äº’ç‰¹å¾èåˆMLP"""
    
    def __init__(self, input_dim, hidden_dims=[512, 256, 128], dropout=0.3):
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„äº¤äº’ç‰¹å¾èåˆMLP
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
            dropout: Dropoutæ¦‚ç‡
        """
        super(OptimizedFusionMLP, self).__init__()
        
        layers = []
        prev_dim = input_dim
        
        for i, hidden_dim in enumerate(hidden_dims):
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim
            
        self.fusion = nn.Sequential(*layers)
        
        # æ·»åŠ æ®‹å·®å—
        self.residual_block = ResidualBlock(hidden_dims[-1], hidden_dims[-1] // 2, dropout)
        
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: æ‹¼æ¥åçš„äº¤äº’ç‰¹å¾å¼ é‡
            
        Returns:
            èåˆåçš„ç‰¹å¾å¼ é‡
        """
        x = self.fusion(x)
        x = self.residual_block(x)
        return x


class OptimizedPOIMatchingMLP(nn.Module):
    """ä¼˜åŒ–çš„POIåŒ¹é…äº¤äº’ç‰¹å¾èåˆMLPæ¨¡å‹"""
    
    def __init__(self, text_dim=384, llm_dim=384, geo_dim=32, gat_dim=128):
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„POIåŒ¹é…æ¨¡å‹
        
        Args:
            text_dim: æ–‡æœ¬ç‰¹å¾ç»´åº¦
            llm_dim: LLMç‰¹å¾ç»´åº¦
            geo_dim: åœ°ç†ç‰¹å¾ç»´åº¦
            gat_dim: GATç‰¹å¾ç»´åº¦
        """
        super(OptimizedPOIMatchingMLP, self).__init__()
        
        # äº¤äº’ç‰¹å¾æå–å™¨
        self.interaction_extractor = OptimizedInteractionExtractor()
        
        # è®¡ç®—äº¤äº’ç‰¹å¾ç»´åº¦
        # æ¯ä¸ªæ¨¡æ€äº¤äº’: å·®å€¼(dim) + ä¹˜ç§¯(dim) + 4ä¸ªæ ‡é‡ç‰¹å¾
        text_interaction_dim = text_dim * 2 + 4
        llm_interaction_dim = llm_dim * 2 + 4
        geo_interaction_dim = geo_dim * 2 + 4
        gat_interaction_dim = gat_dim * 2 + 4
        
        # Text-LLMå¢å¼ºäº¤äº’ç»´åº¦
        enhanced_text_llm_dim = (text_dim + llm_dim) * 2 + 4
        
        total_interaction_dim = text_interaction_dim + llm_interaction_dim + geo_interaction_dim + gat_interaction_dim + enhanced_text_llm_dim
        
        # ç‰¹å¾é™ç»´å±‚ï¼ˆå‡å°‘è¿‡æ‹Ÿåˆé£é™©ï¼‰
        self.feature_reduction = nn.Sequential(
            nn.Linear(total_interaction_dim, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # äº¤äº’ç‰¹å¾èåˆå±‚
        self.fusion_layer = OptimizedFusionMLP(1024, [512, 256, 128], dropout=0.3)
        
        # åˆ†ç±»å±‚
        self.classifier = nn.Sequential(
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
    def forward(self, text_a, text_b, llm_a, llm_b, geo_a, geo_b, gat_a, gat_b):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            text_a: POI Açš„æ–‡æœ¬ç‰¹å¾
            text_b: POI Bçš„æ–‡æœ¬ç‰¹å¾
            llm_a: POI Açš„LLMç‰¹å¾
            llm_b: POI Bçš„LLMç‰¹å¾
            geo_a: POI Açš„åœ°ç†ç‰¹å¾
            geo_b: POI Bçš„åœ°ç†ç‰¹å¾
            gat_a: POI Açš„GATç‰¹å¾
            gat_b: POI Bçš„GATç‰¹å¾
            
        Returns:
            åŒ¹é…æ¦‚ç‡
        """
        # 1. åŸºç¡€äº¤äº’ç‰¹å¾
        text_interaction = self.interaction_extractor(text_a, text_b)
        llm_interaction = self.interaction_extractor(llm_a, llm_b)
        geo_interaction = self.interaction_extractor(geo_a, geo_b)
        gat_interaction = self.interaction_extractor(gat_a, gat_b)
        
        # 2. Text-LLMå¢å¼ºäº¤äº’
        text_llm_concat_a = torch.cat([text_a, llm_a], dim=1)
        text_llm_concat_b = torch.cat([text_b, llm_b], dim=1)
        enhanced_text_llm_interaction = self.interaction_extractor(text_llm_concat_a, text_llm_concat_b)
        
        # 3. æ‹¼æ¥æ‰€æœ‰äº¤äº’ç‰¹å¾
        all_interactions = torch.cat([
            text_interaction,
            llm_interaction, 
            geo_interaction,
            gat_interaction,
            enhanced_text_llm_interaction
        ], dim=1)
        
        # 4. ç‰¹å¾é™ç»´
        reduced_features = self.feature_reduction(all_interactions)
        
        # 5. èåˆå¤„ç†
        fusion_output = self.fusion_layer(reduced_features)
        
        # 6. åˆ†ç±»é¢„æµ‹
        match_prob = self.classifier(fusion_output)
        
        return match_prob


class POIDataset(Dataset):
    """POIåŒ¹é…æ•°æ®é›†"""
    
    def __init__(self, text_a, text_b, llm_a, llm_b, geo_a, geo_b, gat_a, gat_b, labels):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            text_a: POI Açš„æ–‡æœ¬ç‰¹å¾æ•°ç»„
            text_b: POI Bçš„æ–‡æœ¬ç‰¹å¾æ•°ç»„
            llm_a: POI Açš„LLMç‰¹å¾æ•°ç»„
            llm_b: POI Bçš„LLMç‰¹å¾æ•°ç»„
            geo_a: POI Açš„åœ°ç†ç‰¹å¾æ•°ç»„
            geo_b: POI Bçš„åœ°ç†ç‰¹å¾æ•°ç»„
            gat_a: POI Açš„GATç‰¹å¾æ•°ç»„
            gat_b: POI Bçš„GATç‰¹å¾æ•°ç»„
            labels: æ ‡ç­¾æ•°ç»„
        """
        self.text_a = torch.FloatTensor(text_a)
        self.text_b = torch.FloatTensor(text_b)
        self.llm_a = torch.FloatTensor(llm_a)
        self.llm_b = torch.FloatTensor(llm_b)
        self.geo_a = torch.FloatTensor(geo_a)
        self.geo_b = torch.FloatTensor(geo_b)
        self.gat_a = torch.FloatTensor(gat_a)
        self.gat_b = torch.FloatTensor(gat_b)
        self.labels = torch.FloatTensor(labels)
        
    def __len__(self):
        return len(self.labels)
        
    def __getitem__(self, idx):
        return (self.text_a[idx], self.text_b[idx], 
                self.llm_a[idx], self.llm_b[idx],
                self.geo_a[idx], self.geo_b[idx],
                self.gat_a[idx], self.gat_b[idx],
                self.labels[idx])


def load_cache_features():
    """
    åŠ è½½ç¼“å­˜çš„ç‰¹å¾æ•°æ®
    
    Returns:
        cache_features: ç¼“å­˜ç‰¹å¾å­—å…¸
    """
    print("\nğŸ”„ åŠ è½½ç¼“å­˜ç‰¹å¾...")
    
    cache_dir = "outputs/cache"
    cache_features = {}
    
    # åŠ è½½GATåµŒå…¥
    gat_zh_path = os.path.join(cache_dir, "gat_emb_zh.pkl")
    gat_en_path = os.path.join(cache_dir, "gat_emb_en.pkl")
    
    if os.path.exists(gat_zh_path):
        with open(gat_zh_path, 'rb') as f:
            cache_features['gat_emb_zh'] = pickle.load(f)
        print(f"âœ… åŠ è½½GATä¸­æ–‡åµŒå…¥: {len(cache_features['gat_emb_zh'])} ä¸ª")
    
    if os.path.exists(gat_en_path):
        with open(gat_en_path, 'rb') as f:
            cache_features['gat_emb_en'] = pickle.load(f)
        print(f"âœ… åŠ è½½GATè‹±æ–‡åµŒå…¥: {len(cache_features['gat_emb_en'])} ä¸ª")
    
    # åŠ è½½Qwen3å¢å¼ºç‰¹å¾
    enhanced_path = os.path.join(cache_dir, "enhanced_features_huggingface_models_Qwen3-0.6B.pkl")
    if os.path.exists(enhanced_path):
        with open(enhanced_path, 'rb') as f:
            cache_features['enhanced_features_qwen3'] = pickle.load(f)
        print("âœ… åŠ è½½Qwen3å¢å¼ºç‰¹å¾")
    
    return cache_features


def load_poi_data():
    """
    åŠ è½½POIæ•°æ®
    
    Returns:
        bd_df, gd_df: ç™¾åº¦å’Œé«˜å¾·POIæ•°æ®æ¡†
    """
    print("\nğŸ”„ åŠ è½½POIæ•°æ®...")
    
    bd_df = pd.read_csv("data/bd_chinese_data.csv")
    gd_df = pd.read_csv("data/gd_english_data.csv")
    
    print(f"âœ… ç™¾åº¦ä¸­æ–‡POI: {len(bd_df)} ä¸ª")
    print(f"âœ… é«˜å¾·è‹±æ–‡POI: {len(gd_df)} ä¸ª")
    
    return bd_df, gd_df


def load_annotations():
    """
    åŠ è½½æ ‡æ³¨æ•°æ®
    
    Returns:
        annotations_df: åˆå¹¶åçš„æ ‡æ³¨æ•°æ®æ¡†
    """
    print("\nğŸ”„ åŠ è½½æ ‡æ³¨æ•°æ®...")
    
    # åŠ è½½ä¸‰ä¸ªæ ‡æ³¨æ–‡ä»¶
    match1_df = pd.read_csv("data/match1.csv")
    match2_df = pd.read_csv("data/match2.csv") 
    match3_df = pd.read_csv("data/match3.csv")
    
    print(f"âœ… åŠ è½½ data/match1.csv: {len(match1_df)} ä¸ªæ ‡æ³¨")
    print(f"âœ… åŠ è½½ data/match2.csv: {len(match2_df)} ä¸ªæ ‡æ³¨")
    print(f"âœ… åŠ è½½ data/match3.csv: {len(match3_df)} ä¸ªæ ‡æ³¨")
    
    # åˆå¹¶æ‰€æœ‰æ ‡æ³¨æ•°æ®
    annotations_df = pd.concat([match1_df, match2_df, match3_df], ignore_index=True)
    
    print(f"âœ… æ€»æ ‡æ³¨æ•°æ®: {len(annotations_df)} ä¸ª")
    print(f"   - æ­£æ ·æœ¬: {annotations_df['label'].sum()} ä¸ª")
    print(f"   - è´Ÿæ ·æœ¬: {len(annotations_df) - annotations_df['label'].sum()} ä¸ª")
    
    return annotations_df


def split_enhanced_features(enhanced_vector):
    """
    åˆ†å‰²å¢å¼ºç‰¹å¾å‘é‡
    
    Args:
        enhanced_vector: å¢å¼ºç‰¹å¾å‘é‡ (800ç»´)
        
    Returns:
        text_features: æ–‡æœ¬ç‰¹å¾ (384ç»´)
        llm_features: LLMç‰¹å¾ (384ç»´)  
        geo_features: åœ°ç†ç‰¹å¾ (32ç»´)
    """
    if len(enhanced_vector) != 800:
        raise ValueError(f"å¢å¼ºç‰¹å¾å‘é‡é•¿åº¦åº”ä¸º800ï¼Œå®é™…ä¸º{len(enhanced_vector)}")
    
    text_features = enhanced_vector[:384]      # å‰384ç»´æ˜¯æ–‡æœ¬ç‰¹å¾
    llm_features = enhanced_vector[384:768]    # ä¸­é—´384ç»´æ˜¯LLMç‰¹å¾
    geo_features = enhanced_vector[768:800]    # æœ€å32ç»´æ˜¯åœ°ç†ç‰¹å¾
    
    return text_features, llm_features, geo_features


def create_interaction_features(cache_features, bd_df, gd_df, annotations_df):
    """
    åˆ›å»ºäº¤äº’ç‰¹å¾æ•°æ®
    
    Args:
        cache_features: ç¼“å­˜ç‰¹å¾å­—å…¸
        bd_df: ç™¾åº¦POIæ•°æ®æ¡†
        gd_df: é«˜å¾·POIæ•°æ®æ¡†
        annotations_df: æ ‡æ³¨æ•°æ®æ¡†
        
    Returns:
        text_a, text_b, llm_a, llm_b, geo_a, geo_b, gat_a, gat_b, labels: POIå¯¹çš„ç‰¹å¾æ•°ç»„å’Œæ ‡ç­¾
    """
    print("\nğŸ”„ åˆ›å»ºäº¤äº’ç‰¹å¾...")
    
    # è·å–ç¼“å­˜ç‰¹å¾
    enhanced_qwen3 = cache_features.get('enhanced_features_qwen3', {})
    gat_zh_features = cache_features.get('gat_emb_zh', {})  # ç™¾åº¦POIçš„GATç‰¹å¾
    gat_en_features = cache_features.get('gat_emb_en', {})  # é«˜å¾·POIçš„GATç‰¹å¾
    
    # åˆ†ç¦»ç™¾åº¦å’Œé«˜å¾·çš„å¢å¼ºç‰¹å¾
    qwen3_zh_features = enhanced_qwen3.get('zh', {})  # ç™¾åº¦POIçš„Qwen3ç‰¹å¾
    qwen3_en_features = enhanced_qwen3.get('en', {})  # é«˜å¾·POIçš„Qwen3ç‰¹å¾
    
    print(f"Qwen3ç™¾åº¦ç‰¹å¾: {len(qwen3_zh_features)} ä¸ª")
    print(f"Qwen3é«˜å¾·ç‰¹å¾: {len(qwen3_en_features)} ä¸ª")
    print(f"GATç™¾åº¦ç‰¹å¾: {len(gat_zh_features)} ä¸ª")
    print(f"GATé«˜å¾·ç‰¹å¾: {len(gat_en_features)} ä¸ª")
    
    if len(qwen3_zh_features) == 0 or len(qwen3_en_features) == 0:
        print("âŒ å¢å¼ºç‰¹å¾æ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºç‰¹å¾")
        return None, None, None, None, None, None, None, None, None
    
    if len(gat_zh_features) == 0 or len(gat_en_features) == 0:
        print("âŒ GATç‰¹å¾æ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºç‰¹å¾")
        return None, None, None, None, None, None, None, None, None
    
    text_a_list = []
    text_b_list = []
    llm_a_list = []
    llm_b_list = []
    geo_a_list = []
    geo_b_list = []
    gat_a_list = []
    gat_b_list = []
    labels = []
    
    missing_bd_features = 0
    missing_gd_features = 0
    missing_bd_gat = 0
    missing_gd_gat = 0
    successful_matches = 0
    
    # å¤„ç†æ¯ä¸ªæ ‡æ³¨æ ·æœ¬
    for idx, row in tqdm(annotations_df.iterrows(), total=len(annotations_df), desc="åˆ›å»ºäº¤äº’ç‰¹å¾"):
        zh_id = int(row['zh_id'])
        en_id = int(row['en_id'])
        label = int(row['label'])
        
        # è·å–å¢å¼ºç‰¹å¾
        if zh_id not in qwen3_zh_features:
            missing_bd_features += 1
            continue
        if en_id not in qwen3_en_features:
            missing_gd_features += 1
            continue
        
        # è·å–GATç‰¹å¾
        if zh_id not in gat_zh_features:
            missing_bd_gat += 1
            continue
        if en_id not in gat_en_features:
            missing_gd_gat += 1
            continue
            
        bd_enhanced = qwen3_zh_features[zh_id]
        gd_enhanced = qwen3_en_features[en_id]
        bd_gat = gat_zh_features[zh_id]
        gd_gat = gat_en_features[en_id]
        
        # åˆ†å‰²ç‰¹å¾
        bd_text, bd_llm, bd_geo = split_enhanced_features(bd_enhanced)
        gd_text, gd_llm, gd_geo = split_enhanced_features(gd_enhanced)
        
        successful_matches += 1
        
        # ä¿å­˜åŸå§‹ç‰¹å¾å¯¹
        text_a_list.append(bd_text)
        text_b_list.append(gd_text)
        llm_a_list.append(bd_llm)
        llm_b_list.append(gd_llm)
        geo_a_list.append(bd_geo)
        geo_b_list.append(gd_geo)
        gat_a_list.append(bd_gat)
        gat_b_list.append(gd_gat)
        labels.append(label)
    
    print(f"\nğŸ“Š ç‰¹å¾åˆ›å»ºç»Ÿè®¡:")
    print(f"   - ç¼ºå¤±ç™¾åº¦å¢å¼ºç‰¹å¾: {missing_bd_features}")
    print(f"   - ç¼ºå¤±é«˜å¾·å¢å¼ºç‰¹å¾: {missing_gd_features}")
    print(f"   - ç¼ºå¤±ç™¾åº¦GATç‰¹å¾: {missing_bd_gat}")
    print(f"   - ç¼ºå¤±é«˜å¾·GATç‰¹å¾: {missing_gd_gat}")
    print(f"   - æˆåŠŸåˆ›å»ºçš„æ ·æœ¬: {successful_matches}")
    
    if len(text_a_list) == 0:
        print("âŒ æœªèƒ½åˆ›å»ºä»»ä½•ç‰¹å¾å‘é‡")
        return None, None, None, None, None, None, None, None, None
        
    text_a = np.array(text_a_list)
    text_b = np.array(text_b_list)
    llm_a = np.array(llm_a_list)
    llm_b = np.array(llm_b_list)
    geo_a = np.array(geo_a_list)
    geo_b = np.array(geo_b_list)
    gat_a = np.array(gat_a_list)
    gat_b = np.array(gat_b_list)
    labels = np.array(labels, dtype=int)
    
    print("âœ… æˆåŠŸåˆ›å»ºäº¤äº’ç‰¹å¾")
    print(f"   - æ–‡æœ¬ç‰¹å¾Aå½¢çŠ¶: {text_a.shape}")
    print(f"   - æ–‡æœ¬ç‰¹å¾Bå½¢çŠ¶: {text_b.shape}")
    print(f"   - LLMç‰¹å¾Aå½¢çŠ¶: {llm_a.shape}")
    print(f"   - LLMç‰¹å¾Bå½¢çŠ¶: {llm_b.shape}")
    print(f"   - åœ°ç†ç‰¹å¾Aå½¢çŠ¶: {geo_a.shape}")
    print(f"   - åœ°ç†ç‰¹å¾Bå½¢çŠ¶: {geo_b.shape}")
    print(f"   - GATç‰¹å¾Aå½¢çŠ¶: {gat_a.shape}")
    print(f"   - GATç‰¹å¾Bå½¢çŠ¶: {gat_b.shape}")
    print(f"   - æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)}")
    
    return text_a, text_b, llm_a, llm_b, geo_a, geo_b, gat_a, gat_b, labels


def train_model(text_a, text_b, llm_a, llm_b, geo_a, geo_b, gat_a, gat_b, labels, device='cpu'):
    """
    è®­ç»ƒPOIåŒ¹é…æ¨¡å‹
    
    Args:
        text_a, text_b: POIå¯¹çš„æ–‡æœ¬ç‰¹å¾æ•°ç»„
        llm_a, llm_b: POIå¯¹çš„LLMç‰¹å¾æ•°ç»„
        geo_a, geo_b: POIå¯¹çš„åœ°ç†ç‰¹å¾æ•°ç»„
        gat_a, gat_b: POIå¯¹çš„GATç‰¹å¾æ•°ç»„
        labels: æ ‡ç­¾æ•°ç»„
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print("\nğŸ”„ è®­ç»ƒPOIåŒ¹é…æ¨¡å‹...")
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    indices = np.arange(len(labels))
    train_indices, test_indices = train_test_split(
        indices, test_size=0.2, random_state=42, stratify=labels
    )
    
    # è·å–è®­ç»ƒæµ‹è¯•æ•°æ®
    text_a_train, text_a_test = text_a[train_indices], text_a[test_indices]
    text_b_train, text_b_test = text_b[train_indices], text_b[test_indices]
    llm_a_train, llm_a_test = llm_a[train_indices], llm_a[test_indices]
    llm_b_train, llm_b_test = llm_b[train_indices], llm_b[test_indices]
    geo_a_train, geo_a_test = geo_a[train_indices], geo_a[test_indices]
    geo_b_train, geo_b_test = geo_b[train_indices], geo_b[test_indices]
    gat_a_train, gat_a_test = gat_a[train_indices], gat_a[test_indices]
    gat_b_train, gat_b_test = gat_b[train_indices], gat_b[test_indices]
    y_train, y_test = labels[train_indices], labels[test_indices]
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(y_train)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(y_test)}")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
    train_dataset = POIDataset(text_a_train, text_b_train, llm_a_train, llm_b_train, geo_a_train, geo_b_train, gat_a_train, gat_b_train, y_train)
    test_dataset = POIDataset(text_a_test, text_b_test, llm_a_test, llm_b_test, geo_a_test, geo_b_test, gat_a_test, gat_b_test, y_test)
    
    # å¤§å¹…å¢åŠ æ‰¹æ¬¡å¤§å°ä»¥å……åˆ†åˆ©ç”¨GPUå†…å­˜
    train_loader = DataLoader(
        train_dataset, 
        batch_size=512,  # è¿›ä¸€æ­¥å¢åŠ åˆ°512
        shuffle=True, 
        num_workers=4,   # å¤šè¿›ç¨‹æ•°æ®åŠ è½½
        pin_memory=True, # å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸGPUä¼ è¾“
        persistent_workers=True  # ä¿æŒworkerè¿›ç¨‹ï¼Œå‡å°‘å¯åŠ¨å¼€é”€
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=1024,  # æµ‹è¯•æ—¶ç”¨æ›´å¤§çš„æ‰¹æ¬¡
        shuffle=False, 
        num_workers=4, 
        pin_memory=True,
        persistent_workers=True
    )
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = OptimizedPOIMatchingMLP(
        text_dim=text_a.shape[1],
        llm_dim=llm_a.shape[1], 
        geo_dim=geo_a.shape[1],
        gat_dim=gat_a.shape[1]
    ).to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.BCELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)
    
    # è®­ç»ƒå‚æ•°
    num_epochs = 500  # ä¿æŒè¾ƒå¤šçš„è®­ç»ƒè½®æ•°
    best_f1 = 0
    patience = 15
    patience_counter = 0
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        model.train()
        
        # æ·»åŠ è®­ç»ƒè¿›åº¦æ¡
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        for text_a_batch, text_b_batch, llm_a_batch, llm_b_batch, geo_a_batch, geo_b_batch, gat_a_batch, gat_b_batch, labels_batch in train_pbar:
            text_a_batch = text_a_batch.to(device)
            text_b_batch = text_b_batch.to(device)
            llm_a_batch = llm_a_batch.to(device)
            llm_b_batch = llm_b_batch.to(device)
            geo_a_batch = geo_a_batch.to(device)
            geo_b_batch = geo_b_batch.to(device)
            gat_a_batch = gat_a_batch.to(device)
            gat_b_batch = gat_b_batch.to(device)
            labels_batch = labels_batch.to(device).unsqueeze(1)
            
            optimizer.zero_grad()
            outputs = model(text_a_batch, text_b_batch, llm_a_batch, llm_b_batch, geo_a_batch, geo_b_batch, gat_a_batch, gat_b_batch)
            loss = criterion(outputs, labels_batch)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # æ›´æ–°è¿›åº¦æ¡
            train_pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.6f}'})
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # æ¯5ä¸ªepochè¿›è¡Œä¸€æ¬¡éªŒè¯
        if (epoch + 1) % 5 == 0:
            model.eval()
            val_predictions = []
            val_probabilities = []
            val_labels = []
            
            with torch.no_grad():
                for text_a_batch, text_b_batch, llm_a_batch, llm_b_batch, geo_a_batch, geo_b_batch, gat_a_batch, gat_b_batch, labels_batch in test_loader:
                    text_a_batch = text_a_batch.to(device)
                    text_b_batch = text_b_batch.to(device)
                    llm_a_batch = llm_a_batch.to(device)
                    llm_b_batch = llm_b_batch.to(device)
                    geo_a_batch = geo_a_batch.to(device)
                    geo_b_batch = geo_b_batch.to(device)
                    gat_a_batch = gat_a_batch.to(device)
                    gat_b_batch = gat_b_batch.to(device)
                    
                    outputs = model(text_a_batch, text_b_batch, llm_a_batch, llm_b_batch, geo_a_batch, geo_b_batch, gat_a_batch, gat_b_batch)
                    predictions = (outputs > 0.5).float().cpu().numpy()
                    probabilities = outputs.cpu().numpy()
                    
                    val_predictions.extend(predictions.flatten())
                    val_probabilities.extend(probabilities.flatten())
                    val_labels.extend(labels_batch.cpu().numpy())
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            val_f1 = f1_score(val_labels, val_predictions)
            val_auc = roc_auc_score(val_labels, val_probabilities)
            val_acc = accuracy_score(val_labels, val_predictions)
            
            print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f} - Val F1: {val_f1:.4f} - Val AUC: {val_auc:.4f} - Val Acc: {val_acc:.4f}")
            
            # æ—©åœæ£€æŸ¥
            if val_f1 > best_f1:
                best_f1 = val_f1
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(model.state_dict(), 'outputs/best_optimized_interaction_fusion_model.pth')
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                print(f"æ—©åœï¼šéªŒè¯F1åˆ†æ•°è¿ç»­{patience}ä¸ªepochæœªæå‡")
                break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    model.load_state_dict(torch.load('outputs/best_optimized_interaction_fusion_model.pth'))
    model.eval()
    
    final_predictions = []
    final_probabilities = []
    final_labels = []
    
    with torch.no_grad():
        for text_a_batch, text_b_batch, llm_a_batch, llm_b_batch, geo_a_batch, geo_b_batch, gat_a_batch, gat_b_batch, labels_batch in test_loader:
            text_a_batch = text_a_batch.to(device)
            text_b_batch = text_b_batch.to(device)
            llm_a_batch = llm_a_batch.to(device)
            llm_b_batch = llm_b_batch.to(device)
            geo_a_batch = geo_a_batch.to(device)
            geo_b_batch = geo_b_batch.to(device)
            gat_a_batch = gat_a_batch.to(device)
            gat_b_batch = gat_b_batch.to(device)
            
            outputs = model(text_a_batch, text_b_batch, llm_a_batch, llm_b_batch, geo_a_batch, geo_b_batch, gat_a_batch, gat_b_batch)
            predictions = (outputs > 0.5).float().cpu().numpy()
            probabilities = outputs.cpu().numpy()
            
            final_predictions.extend(predictions.flatten())
            final_probabilities.extend(probabilities.flatten())
            final_labels.extend(labels_batch.cpu().numpy())
    
    # æœ€ç»ˆè¯„ä¼°
    final_f1 = f1_score(final_labels, final_predictions)
    final_auc = roc_auc_score(final_labels, final_probabilities)
    final_acc = accuracy_score(final_labels, final_predictions)
    final_precision = precision_score(final_labels, final_predictions)
    final_recall = recall_score(final_labels, final_predictions)
    
    print(f"\nğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(f"   - F1åˆ†æ•°: {final_f1:.4f}")
    print(f"   - AUCåˆ†æ•°: {final_auc:.4f}")
    print(f"   - å‡†ç¡®ç‡: {final_acc:.4f}")
    print(f"   - ç²¾ç¡®ç‡: {final_precision:.4f}")
    print(f"   - å¬å›ç‡: {final_recall:.4f}")
    
    results = {
        'f1_score': final_f1,
        'auc_score': final_auc,
        'accuracy': final_acc,
        'precision': final_precision,
        'recall': final_recall
    }
    
    return model, results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–äº¤äº’ç‰¹å¾èåˆMLPå®éªŒ V2")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    cache_features = load_cache_features()
    bd_df, gd_df = load_poi_data()
    annotations_df = load_annotations()
    
    if not cache_features:
        print("âŒ æ— æ³•åŠ è½½ç¼“å­˜ç‰¹å¾ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # åˆ›å»ºäº¤äº’ç‰¹å¾
    text_a, text_b, llm_a, llm_b, geo_a, geo_b, gat_a, gat_b, labels = create_interaction_features(
        cache_features, bd_df, gd_df, annotations_df
    )
    
    if text_a is None:
        print("âŒ æ— æ³•åˆ›å»ºäº¤äº’ç‰¹å¾ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # è®­ç»ƒæ¨¡å‹
    model, results = train_model(text_a, text_b, llm_a, llm_b, geo_a, geo_b, gat_a, gat_b, labels, device)
    
    print("\nâœ… ä¼˜åŒ–äº¤äº’ç‰¹å¾èåˆMLPå®éªŒ V2 å®Œæˆ")
    print(f"æœ€ç»ˆç»“æœ: F1={results['f1_score']:.4f}, AUC={results['auc_score']:.4f}, Acc={results['accuracy']:.4f}")


if __name__ == "__main__":
    main()