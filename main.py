import pandas as pd
from sentence_transformers import SentenceTransformer
import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
import pickle
import os
import geohash2
from tqdm import tqdm
import numpy as np
import warnings
from haversine import haversine
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score
import lightgbm as lgb
# ä½¿ç”¨BallTreeè¿›è¡Œé«˜æ•ˆèŒƒå›´æŸ¥è¯¢
from sklearn.neighbors import BallTree
from sklearn.ensemble import RandomForestClassifier
import sys
import argparse
from datetime import datetime

# å¯¼å…¥LLMæ–‡æœ¬å¢å¼ºæ¨¡å—
from models.llm_text_enhancer import LLMTextEnhancer

warnings.filterwarnings("ignore", category=UserWarning, module='torch_geometric.nn.conv.gatv2_conv')

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='POIåŒ¹é…ç³»ç»Ÿ - æ”¯æŒå¤šç§LLMæ¨¡å‹')
    
    # LLMæ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument('--model-type', type=str, default='huggingface', 
                       choices=['huggingface', 'ollama'],
                       help='LLMæ¨¡å‹ç±»å‹ (é»˜è®¤: huggingface)')
    
    parser.add_argument('--model-name', type=str, default='models/DialoGPT-small',
                       help='æ¨¡å‹åç§°æˆ–è·¯å¾„ (é»˜è®¤: models/DialoGPT-small)')
    
    parser.add_argument('--ollama-url', type=str, default='http://localhost:11434',
                       help='OLLAMAæœåŠ¡URL (é»˜è®¤: http://localhost:11434)')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--debug', action='store_true', default=False,
                       help='å¯ç”¨debugæ¨¡å¼ï¼Œåªå¤„ç†å°‘é‡æ•°æ®')
    
    parser.add_argument('--debug-limit', type=int, default=1000,
                       help='debugæ¨¡å¼ä¸‹çš„æ•°æ®é™åˆ¶ (é»˜è®¤: 1000)')
    
    parser.add_argument('--device', type=str, default='cuda',
                       choices=['cuda', 'cpu'],
                       help='è®¡ç®—è®¾å¤‡ (é»˜è®¤: cuda)')
    
    return parser.parse_args()

# è§£æå‘½ä»¤è¡Œå‚æ•°
args = parse_arguments()

# é¡¹ç›®è·¯å¾„é…ç½®
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(PROJECT_ROOT, 'data')
OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs')
MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')
os.makedirs(OUTPUT_DIR, exist_ok=True)

# è¾“å…¥æ•°æ®
BD_CHINESE_POI_PATH = os.path.join(DATA_DIR, 'bd_chinese_data.csv')  # zhå¯¹åº”ç™¾åº¦
GD_CHINESE_POI_PATH = os.path.join(DATA_DIR, 'gd_chinese_data.csv')  # enå¯¹åº”é«˜å¾·
LABELED_DATA_PATH = os.path.join(DATA_DIR, 'match2.csv')  # åŒ…å«en_id(é«˜å¾·), zh_id(ç™¾åº¦), label

# é…ç½®å‚æ•° - ä»å‘½ä»¤è¡Œå‚æ•°è·å–
DEBUG_MODE = args.debug  # ä»å‘½ä»¤è¡Œå‚æ•°è·å–
DEBUG_DATA_LIMIT = args.debug_limit  # ä»å‘½ä»¤è¡Œå‚æ•°è·å–

# LLMæ¨¡å‹é…ç½®
LLM_MODEL_TYPE = args.model_type
LLM_MODEL_NAME = args.model_name
OLLAMA_BASE_URL = args.ollama_url
DEVICE = args.device

MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'
MODEL_PATH = os.path.join(MODEL_DIR, MODEL_NAME)

# è¾“å‡ºæ–‡ä»¶è·¯å¾„
FINAL_MATCHES_PATH = os.path.join(OUTPUT_DIR, 'final_matched_pairs.csv')

CLASSIFIER_MODEL_PATH = os.path.join(OUTPUT_DIR, 'gat_classifier_model.pkl')
EVALUATION_REPORT_PATH = os.path.join(OUTPUT_DIR, 'model_evaluation_report.txt')

# ç¼“å­˜è·¯å¾„é…ç½®
CACHE_DIR = os.path.join(OUTPUT_DIR, 'cache')
os.makedirs(CACHE_DIR, exist_ok=True)
TEXT_EMBEDDINGS_CACHE = os.path.join(CACHE_DIR, 'text_embeddings.pkl')
GAT_EMBEDDINGS_CACHE = os.path.join(CACHE_DIR, 'gat_embeddings.pkl')
ENHANCED_FEATURES_CACHE = os.path.join(CACHE_DIR, 'enhanced_features.pkl')

# --- æ¨¡å‹ä¸å¤„ç†å‚æ•° ---
TEXT_EMBEDDING_BATCH_SIZE = 256
GEOHASH_PRECISION = 7  # çº¦ç­‰äº 153m x 153m çš„æ ¼å­
GAT_OUTPUT_DIM = 128   # GATè¾“å‡ºçš„åµŒå…¥ç»´åº¦
MATCHING_BATCH_SIZE = 1024

# --- GAT æ¨¡å‹å®šä¹‰ ---
class GAT(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = GATConv(in_channels, 32, heads=8, dropout=0.2)
        self.conv2 = GATConv(32 * 8, out_channels, heads=1, concat=False, dropout=0.2)

    def forward(self, x, edge_index):
        x = F.dropout(x, p=0.2, training=self.training)
        x = F.elu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.2, training=self.training)
        x = self.conv2(x, edge_index)
        return x

# --- 1. ç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬åµŒå…¥ ---
def generate_text_embeddings(file_path, model, device):
    print(f"\n--- æ­¥éª¤ 1: å¼€å§‹ä¸º {os.path.basename(file_path)} ç”Ÿæˆæ–‡æœ¬åµŒå…¥ ---")
    df = pd.read_csv(file_path)
    
    if 'id' not in df.columns:
        print(f"è­¦å‘Š: åœ¨ {os.path.basename(file_path)} ä¸­æœªæ‰¾åˆ° 'id' åˆ—ã€‚å°†è‡ªåŠ¨ç”ŸæˆåŸºäºè¡Œå·çš„IDã€‚")
        df['id'] = range(len(df))
    
    required_cols = ['name', 'category', 'latitude', 'longitude']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"é”™è¯¯: æ–‡ä»¶ {os.path.basename(file_path)} ä¸­ç¼ºå°‘ä»¥ä¸‹å¿…è¦åˆ—: {missing_cols}")
        return pd.DataFrame(), {}
    
    df.dropna(subset=['id', 'name', 'address', 'category', 'latitude', 'longitude'], inplace=True)
    df['id'] = df['id'].astype(int)
    
    # Debugæ¨¡å¼ï¼šæ™ºèƒ½é€‰æ‹©èƒ½å¤Ÿå½¢æˆå®Œæ•´åŒ¹é…å¯¹çš„æ•°æ®
    if DEBUG_MODE:
        original_len = len(df)
        
        # è¯»å–æ ‡æ³¨æ•°æ®
        labeled_data = pd.read_csv(LABELED_DATA_PATH)
        
        # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ˜¯ç™¾åº¦è¿˜æ˜¯é«˜å¾·æ•°æ®
        if 'bd_chinese' in file_path:
            # ç™¾åº¦æ•°æ®ï¼Œè·å–zh_id
            # é¦–å…ˆé€‰æ‹©å‰DEBUG_DATA_LIMITä¸ªæ ‡æ³¨å¯¹ï¼Œç„¶åæå–å¯¹åº”çš„ç™¾åº¦ID
            selected_pairs = labeled_data.head(DEBUG_DATA_LIMIT)
            required_ids = set(selected_pairs['zh_id'].unique())
            data_type = "ç™¾åº¦"
        else:
            # é«˜å¾·æ•°æ®ï¼Œè·å–en_id
            # é€‰æ‹©ç›¸åŒçš„æ ‡æ³¨å¯¹ï¼Œæå–å¯¹åº”çš„é«˜å¾·ID
            selected_pairs = labeled_data.head(DEBUG_DATA_LIMIT)
            required_ids = set(selected_pairs['en_id'].unique())
            data_type = "é«˜å¾·"
        
        # ç­›é€‰åŒ…å«æ ‡æ³¨æ•°æ®IDçš„è¡Œ
        df_required = df[df['id'].isin(required_ids)]
        
        # å¦‚æœæ‰¾åˆ°çš„POIæ•°é‡ä¸è¶³ï¼Œè¡¥å……ä¸€äº›å…¶ä»–æ•°æ®
        if len(df_required) < DEBUG_DATA_LIMIT:
            remaining_limit = DEBUG_DATA_LIMIT - len(df_required)
            df_other = df[~df['id'].isin(required_ids)].head(remaining_limit)
            df = pd.concat([df_required, df_other], ignore_index=True)
            print(f"ğŸ› DEBUGæ¨¡å¼({data_type}): åŒ…å« {len(df_required)} ä¸ªåŒ¹é…å¯¹ç›¸å…³POI + {len(df_other)} ä¸ªå…¶ä»–POIï¼Œå…± {len(df)} æ¡")
        else:
            df = df_required.head(DEBUG_DATA_LIMIT)
            print(f"ğŸ› DEBUGæ¨¡å¼({data_type}): é€‰æ‹© {len(df)} ä¸ªåŒ¹é…å¯¹ç›¸å…³POI")
        
        print(f"   åŸå§‹æ•°æ®: {original_len} æ¡ -> DEBUGå¤„ç†: {len(df)} æ¡")
        print(f"   é€‰æ‹©çš„{data_type}IDç¤ºä¾‹: {list(df['id'].head(10))}")
    
    # æ‹¼æ¥æ–‡æœ¬
    df['text_to_embed'] = df['name'].astype(str) + " | " + df['category'].astype(str) + " | " + df['address'].astype(str)
    
    print(f"å…± {len(df)} æ¡æœ‰æ•ˆPOIã€‚")
    embeddings = model.encode(
        df['text_to_embed'].tolist(),
        batch_size=TEXT_EMBEDDING_BATCH_SIZE,
        show_progress_bar=True,
        device=device,
        convert_to_numpy=True
    )
    
    embeddings_dict = {row['id']: emb for _, row, emb in zip(range(len(df)), df.to_dict('records'), embeddings)}
    
    return df, embeddings_dict

import torch
import torch.nn as nn

# --- 2. æ·»åŠ Geohashç¼–ç  ---
def add_geohash(df):
    print("\n--- æ­¥éª¤ 2: æ·»åŠ Geohashç¼–ç  ---")

     # æ£€æŸ¥DataFrameæ˜¯å¦ä¸ºç©º
    if df.empty:
        print("è­¦å‘Š: DataFrameä¸ºç©ºï¼Œæ— æ³•æ·»åŠ Geohash")
        return df
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç»çº¬åº¦åˆ—
    if 'latitude' not in df.columns or 'longitude' not in df.columns:
        print("è­¦å‘Š: DataFrameç¼ºå°‘ç»çº¬åº¦åˆ—ï¼Œæ— æ³•æ·»åŠ Geohash")
        return df
    
    df['geohash'] = df.apply(
        lambda row: geohash2.encode(row['latitude'], row['longitude'], precision=GEOHASH_PRECISION),
        axis=1
    )
    return df

# --- GeohashåµŒå…¥ --- 
class GeohashEncoder(nn.Module):
    def __init__(self, charset_size, embedding_dim, hidden_dim, output_dim, device):
        """
        ä½¿ç”¨BiGRUç¼–ç Geohashå­—ç¬¦ä¸²
        :param charset_size: Geohashå­—ç¬¦é›†å¤§å°ï¼ˆBase32å…±32ä¸ªå­—ç¬¦ï¼‰
        :param embedding_dim: å­—ç¬¦åµŒå…¥ç»´åº¦
        :param hidden_dim: GRUéšè—å±‚ç»´åº¦
        :param output_dim: æœ€ç»ˆè¾“å‡ºç»´åº¦
        :param device: è®¡ç®—è®¾å¤‡
        """
        super(GeohashEncoder, self).__init__()
        self.device = device
        self.hidden_dim = hidden_dim
        
        # å­—ç¬¦åµŒå…¥å±‚
        self.embedding = nn.Embedding(charset_size, embedding_dim).to(device)
        
        # åŒå‘GRU
        self.bigru = nn.GRU(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            batch_first=True,
            bidirectional=True
        ).to(device)
        
        # è¾“å‡ºå±‚
        self.fc = nn.Linear(hidden_dim * 2, output_dim).to(device)  # åŒå‘æ‰€ä»¥æ˜¯2å€
        
    def forward(self, geohash_indices):
        """
        :param geohash_indices: å·²è½¬æ¢ä¸ºç´¢å¼•åºåˆ—çš„Geohash Tensor
        :return: Geohashçš„åµŒå…¥è¡¨ç¤º
        """
        # å­—ç¬¦åµŒå…¥
        embedded = self.embedding(geohash_indices)
        
        # é€šè¿‡åŒå‘GRU
        gru_output, hidden = self.bigru(embedded)
        
        # å–æœ€åæ—¶é—´æ­¥çš„å‰å‘å’Œåå‘éšè—çŠ¶æ€ï¼Œæ‹¼æ¥åé€šè¿‡å…¨è¿æ¥å±‚
        forward_last = hidden[-2, :, :]  # å‰å‘æœ€åéšè—çŠ¶æ€
        backward_last = hidden[-1, :, :]  # åå‘æœ€åéšè—çŠ¶æ€
        combined = torch.cat((forward_last, backward_last), dim=1)
        
        output = self.fc(combined)
        return output


# --- 3. æ„å»ºå›¾å¹¶ç”ŸæˆGATåµŒå…¥(ä½¿ç”¨ç±»åˆ«çš„onehotç¼–ç ä½œä¸ºèŠ‚ç‚¹) ---
def generate_gat_embeddings(df, device):
    # ä½¿ç”¨Kâ€”nnæ„å›¾
    print("\n--- æ­¥éª¤ 3: æ„å»ºå›¾å¹¶ç”ŸæˆGATåµŒå…¥ (ä½¿ç”¨Ké‚»è¿‘æ„å›¾) ---")

    # å‡†å¤‡èŠ‚ç‚¹ç‰¹å¾ - ä½¿ç”¨ç±»åˆ«ä¿¡æ¯
    print("å‡†å¤‡èŠ‚ç‚¹ç‰¹å¾ (ä½¿ç”¨ç±»åˆ«ä¿¡æ¯)...")

    # åˆ›å»ºç±»åˆ«åˆ°ç´¢å¼•çš„æ˜ å°„
    categories = df['category'].unique()
    cat_to_idx = {cat: idx for idx, cat in enumerate(categories)}

    # åˆ›å»ºèŠ‚ç‚¹ç‰¹å¾ - ä½¿ç”¨ç±»åˆ«ç´¢å¼•
    ids = df['id'].tolist()
    node_features = np.array([cat_to_idx[row['category']] for _, row in df.iterrows()])

    # å°†ç±»åˆ«ç´¢å¼•è½¬æ¢ä¸ºone-hotç¼–ç 
    num_categories = len(categories)
    node_features = np.eye(num_categories)[node_features].astype('float32')

    x = torch.tensor(node_features).to(device)

    # æ„å»ºå›¾ï¼šåŸºäºKé‚»è¿‘è¿æ¥èŠ‚ç‚¹
    print("æ„å»ºKé‚»è¿‘å›¾...")
    df['node_idx'] = range(len(df))

    # æå–åæ ‡å¹¶è½¬æ¢ä¸ºnumpyæ•°ç»„
    coords = df[['latitude', 'longitude']].values

    # ä½¿ç”¨BallTreeè¿›è¡Œé«˜æ•ˆKé‚»è¿‘æŸ¥è¯¢
    from sklearn.neighbors import BallTree
    tree = BallTree(coords, metric='haversine')

    # æŸ¥è¯¢æ¯ä¸ªç‚¹çš„10ä¸ªæœ€è¿‘é‚»å±…ï¼ˆåŒ…æ‹¬è‡ªèº«ï¼‰
    K = 5
    distances, indices = tree.query(coords, k=K+1)  # +1 å› ä¸ºåŒ…æ‹¬è‡ªèº«

    source_nodes, target_nodes = [], []

    # éå†æ¯ä¸ªèŠ‚ç‚¹ï¼Œæ·»åŠ ä¸é‚»å±…çš„è¾¹
    for i in tqdm(range(len(indices)), desc="æ„å»ºè¾¹"):
        # è·³è¿‡è‡ªèº«ï¼ˆç´¢å¼•0æ˜¯è‡ªèº«ï¼‰
        for j in indices[i][1:]:
            source_nodes.append(i)
            target_nodes.append(j)
            # æ·»åŠ åå‘è¾¹ï¼ˆæ— å‘å›¾ï¼‰
            source_nodes.append(j)
            target_nodes.append(i)

    edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)
    print(f"å›¾æ„å»ºå®Œæˆï¼Œå…± {len(ids)} ä¸ªèŠ‚ç‚¹, {edge_index.size(1)} æ¡è¾¹ã€‚")

    # è¿è¡ŒGATæ¨¡å‹ - ä½¿ç”¨CPUé¿å…GPUå†…å­˜ä¸è¶³
    print("ç”±äºèŠ‚ç‚¹æ•°é‡è¾ƒå¤§ï¼Œä½¿ç”¨CPUå¤„ç†GATæ¨¡å‹ä»¥é¿å…GPUå†…å­˜ä¸è¶³...")
    x_cpu = x.cpu()
    edge_index_cpu = edge_index
    data = Data(x=x_cpu, edge_index=edge_index_cpu)
    model = GAT(in_channels=x_cpu.size(1), out_channels=GAT_OUTPUT_DIM)  # ä¿æŒåœ¨CPU
    model.eval()

    print("é€šè¿‡GATæ¨¡å‹ç”Ÿæˆæœ€ç»ˆåµŒå…¥...")
    with torch.no_grad():
        gat_embeddings_tensor = model(data.x, data.edge_index)

    gat_embeddings = gat_embeddings_tensor.cpu().numpy()
    gat_embeddings_dict = {id: emb for id, emb in zip(ids, gat_embeddings)}

    return gat_embeddings_dict


# --- 4. è·å¾—æ‰€æœ‰åµŒå…¥ ---
def create_poi_enhanced_features_from_cache(df, cache_dir, debug_mode=False, debug_limit=1000, model_name="DialoGPT-small"):
    """
    ä»ç¼“å­˜æ–‡ä»¶ä¸­åŠ è½½å¹¶åˆ›å»ºåŠ æƒèåˆçš„å¢å¼ºç‰¹å¾å‘é‡
    
    Args:
        df: POIæ•°æ®æ¡†
        cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        debug_mode: æ˜¯å¦ä¸ºè°ƒè¯•æ¨¡å¼
        debug_limit: è°ƒè¯•æ¨¡å¼ä¸‹çš„æ•°æ®é™åˆ¶
        model_name: LLMæ¨¡å‹åç§°
    
    Returns:
        enhanced_features: å¢å¼ºç‰¹å¾å­—å…¸
    """
    enhanced_features = {}
    
    # æ„å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„
    debug_suffix = f"_debug_{debug_limit}" if debug_mode else ""
    model_suffix = f"_huggingface_models_{model_name.replace('/', '_')}"
    
    # æ–‡ä»¶è·¯å¾„
    text_emb_file = os.path.join(cache_dir, f"text_emb_zh{debug_suffix}.pkl")
    gat_emb_file = os.path.join(cache_dir, f"gat_emb_zh{debug_suffix}.pkl") 
    enhanced_file = os.path.join(cache_dir, f"enhanced_features{debug_suffix}{model_suffix}.pkl")
    
    print(f"ä»ç¼“å­˜åŠ è½½ç‰¹å¾æ–‡ä»¶...")
    print(f"æ–‡æœ¬åµŒå…¥: {text_emb_file}")
    print(f"GATåµŒå…¥: {gat_emb_file}")
    print(f"å¢å¼ºç‰¹å¾: {enhanced_file}")
    
    # åŠ è½½å„ç§ç‰¹å¾
    try:
        with open(text_emb_file, 'rb') as f:
            text_emb_dict = pickle.load(f)
        print(f"âœ“ æ–‡æœ¬åµŒå…¥åŠ è½½å®Œæˆ: {len(text_emb_dict)} ä¸ªç‰¹å¾")
        
        with open(gat_emb_file, 'rb') as f:
            gat_emb_dict = pickle.load(f)
        print(f"âœ“ GATåµŒå…¥åŠ è½½å®Œæˆ: {len(gat_emb_dict)} ä¸ªç‰¹å¾")
        
        with open(enhanced_file, 'rb') as f:
            enhanced_features_cache = pickle.load(f)
        print(f"âœ“ å¢å¼ºç‰¹å¾åŠ è½½å®Œæˆ: {len(enhanced_features_cache)} ä¸ªç‰¹å¾")
        
    except FileNotFoundError as e:
        print(f"ç¼“å­˜æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        return None
    
    # è§£è€¦ç‰¹å¾å¹¶å®ç°åŠ æƒèåˆ
    print("\nå¼€å§‹ç‰¹å¾è§£è€¦å’ŒåŠ æƒèåˆ...")
    
    # ç‰¹å¾æƒé‡è®¾ç½®ï¼ˆç»çº¬åº¦ > LLM > æ–‡æœ¬ï¼‰
    geohash_weight = 0.5    # ç»çº¬åº¦ç‰¹å¾æƒé‡æœ€é«˜
    llm_weight = 0.3        # LLMç‰¹å¾æƒé‡æ¬¡ä¹‹
    text_weight = 0.15      # æ–‡æœ¬ç‰¹å¾æƒé‡è¾ƒä½
    gat_weight = 0.05       # GATç‰¹å¾æƒé‡æœ€ä½
    
    print(f"ç‰¹å¾æƒé‡è®¾ç½®:")
    print(f"  - Geohash (ç»çº¬åº¦): {geohash_weight}")
    print(f"  - LLMç‰¹å¾: {llm_weight}")
    print(f"  - æ–‡æœ¬ç‰¹å¾: {text_weight}")
    print(f"  - GATç‰¹å¾: {gat_weight}")
    
    for _, row in tqdm(df.iterrows(), total=len(df), desc="æ„å»ºåŠ æƒèåˆç‰¹å¾"):
        poi_id = row['id']
        
        if poi_id not in enhanced_features_cache:
            continue
            
        # ä»ç¼“å­˜çš„å¢å¼ºç‰¹å¾ä¸­è§£è€¦å„éƒ¨åˆ†
        cached_feature = enhanced_features_cache[poi_id]
        
        # è·å–å„ä¸ªç‰¹å¾ç»„ä»¶
        text_vector = text_emb_dict.get(poi_id)
        gat_vector = gat_emb_dict.get(poi_id)
        
        if text_vector is None or gat_vector is None:
            continue
        
        # ä»ç¼“å­˜ç‰¹å¾ä¸­æå–å„éƒ¨åˆ†
        text_dim = len(text_vector)
        gat_dim = len(gat_vector)
        
        # å‡è®¾ç¼“å­˜ç‰¹å¾çš„ç»“æ„æ˜¯: [text, geohash, llm]
        geohash_start = text_dim
        geohash_end = geohash_start + 64  # Geohashç¼–ç å™¨è¾“å‡º64ç»´
        llm_start = geohash_end
        
        # æå–å„éƒ¨åˆ†ç‰¹å¾
        cached_text = cached_feature[:text_dim]
        cached_geohash = cached_feature[geohash_start:geohash_end]
        cached_llm = cached_feature[llm_start:] if llm_start < len(cached_feature) else np.array([])
        
        # æ ‡å‡†åŒ–å„ç‰¹å¾åˆ°ç›¸åŒå°ºåº¦
        def normalize_feature(feat):
            if len(feat) == 0:
                return feat
            norm = np.linalg.norm(feat)
            return feat / norm if norm > 0 else feat
        
        norm_text = normalize_feature(cached_text)
        norm_geohash = normalize_feature(cached_geohash)
        norm_gat = normalize_feature(gat_vector)
        norm_llm = normalize_feature(cached_llm) if len(cached_llm) > 0 else np.array([])
        
        # åŠ æƒèåˆ
        weighted_features = []
        
        # æ·»åŠ åŠ æƒçš„å„ä¸ªç‰¹å¾
        if len(norm_geohash) > 0:
            weighted_features.append(norm_geohash * geohash_weight)
        
        if len(norm_llm) > 0:
            weighted_features.append(norm_llm * llm_weight)
            
        if len(norm_text) > 0:
            weighted_features.append(norm_text * text_weight)
            
        if len(norm_gat) > 0:
            weighted_features.append(norm_gat * gat_weight)
        
        # æ‹¼æ¥æ‰€æœ‰åŠ æƒç‰¹å¾
        if weighted_features:
            combined_vector = np.concatenate(weighted_features)
            enhanced_features[poi_id] = combined_vector
    
    print(f"âœ“ åŠ æƒèåˆç‰¹å¾æ„å»ºå®Œæˆ: {len(enhanced_features)} ä¸ªç‰¹å¾")
    return enhanced_features


def create_poi_enhanced_features(df, text_emb_dict, gat_emb_dict, geohash_encoder, device, llm_enhancer=None):
    """
    ä¸ºæ¯ä¸ªPOIåˆ›å»ºå¢å¼ºç‰¹å¾å‘é‡ï¼šæ‹¼æ¥æ–‡æœ¬ã€Geohashï¼ˆBiGRUç¼–ç ï¼‰ã€å›¾å‘é‡å’ŒLLMå¢å¼ºç‰¹å¾
    
    Args:
        df: POIæ•°æ®æ¡†
        text_emb_dict: æ–‡æœ¬åµŒå…¥å­—å…¸
        gat_emb_dict: GATåµŒå…¥å­—å…¸
        geohash_encoder: Geohashç¼–ç å™¨
        device: è®¡ç®—è®¾å¤‡
        llm_enhancer: LLMæ–‡æœ¬å¢å¼ºå™¨ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        enhanced_features: å¢å¼ºç‰¹å¾å­—å…¸
    """
    enhanced_features = {}
    
    # Base32å­—ç¬¦é›†ï¼šGeohashä½¿ç”¨çš„32ä¸ªå­—ç¬¦
    base32_chars = "0123456789bcdefghjkmnpqrstuvwxyz"
    char_to_idx = {char: idx for idx, char in enumerate(base32_chars)}
    
    # å¦‚æœæä¾›äº†LLMå¢å¼ºå™¨ï¼Œæ‰¹é‡ç”ŸæˆLLMå¢å¼ºç‰¹å¾
    llm_enhanced_features = {}
    if llm_enhancer is not None:
        print("ç”ŸæˆLLMå¢å¼ºç‰¹å¾...")
        poi_texts = []
        poi_ids = []
        
        for _, row in tqdm(df.iterrows(), total=len(df), desc="å‡†å¤‡POIæ•°æ®"):
            poi_id = row['id']
            poi_name = str(row.get('name', ''))
            poi_address = str(row.get('address', ''))
            poi_category = str(row.get('category', ''))
            
            poi_texts.append({
                'name': poi_name,
                'address': poi_address,
                'category': poi_category
            })
            poi_ids.append(poi_id)
        
        # æ‰¹é‡ç”ŸæˆLLMå¢å¼ºç‰¹å¾
        try:
            # å°†POIå­—å…¸è½¬æ¢ä¸ºæ–‡æœ¬åˆ—è¡¨
            poi_text_list = []
            for poi_dict in poi_texts:
                poi_text = f"{poi_dict['name']} | {poi_dict['category']} | {poi_dict['address']}"
                poi_text_list.append(poi_text)
            
            print(f"å¼€å§‹ç”Ÿæˆ {len(poi_text_list)} ä¸ªPOIçš„LLMå¢å¼ºç‰¹å¾...")
            llm_features = llm_enhancer.batch_generate_enhanced_features(poi_text_list)
            for poi_id, llm_feat in zip(poi_ids, llm_features):
                llm_enhanced_features[poi_id] = llm_feat
            print(f"LLMå¢å¼ºç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œå…± {len(llm_enhanced_features)} ä¸ªç‰¹å¾")
        except Exception as e:
            print(f"LLMå¢å¼ºç‰¹å¾ç”Ÿæˆå¤±è´¥: {e}")
            print("å°†ä½¿ç”¨åŸå§‹ç‰¹å¾ç»§ç»­...")
    
    for _, row in df.iterrows():
        poi_id = row['id']
        
        # 1. è·å–æ–‡æœ¬å‘é‡
        text_vector = text_emb_dict[poi_id]
        
        # 2. è·å–GATå›¾å‘é‡
        gat_vector = gat_emb_dict[poi_id]
        
        # 3. ç”ŸæˆGeohashå‘é‡ï¼ˆä½¿ç”¨BiGRUç¼–ç ï¼‰
        geohash_str = row['geohash']
        
        # å°†Geohashå­—ç¬¦ä¸²è½¬æ¢ä¸ºç´¢å¼•åºåˆ—
        geohash_indices = [char_to_idx.get(char, 0) for char in geohash_str]
        geohash_indices = torch.tensor(geohash_indices, dtype=torch.long).unsqueeze(0).to(device)
        
        # é€šè¿‡BiGRUç¼–ç å™¨è·å–Geohashå‘é‡
        with torch.no_grad():
            geohash_vector = geohash_encoder(geohash_indices).squeeze().cpu().numpy()
        
        # 4. è·å–LLMå¢å¼ºç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if poi_id in llm_enhanced_features:
            llm_vector = llm_enhanced_features[poi_id]
            # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾ï¼šåŸå§‹æ–‡æœ¬ + Geohash + LLMå¢å¼ºç‰¹å¾
            combined_vector = np.concatenate([text_vector, geohash_vector, llm_vector])
        else:
            # å¦‚æœæ²¡æœ‰LLMå¢å¼ºç‰¹å¾ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾
            combined_vector = np.concatenate([text_vector, geohash_vector])
 
        enhanced_features[poi_id] = combined_vector
    
    return enhanced_features


def train_classifier_with_enhanced_features(enhanced_feat_en, enhanced_feat_zh, labeled_data_path):
    print("\n--- æ­¥éª¤ 4: ä½¿ç”¨å¢å¼ºç‰¹å¾è®­ç»ƒåˆ†ç±»å™¨ ---")
    
    # åŠ è½½æ ‡æ³¨æ•°æ®
    labeled_data = pd.read_csv(labeled_data_path)
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    features = []
    labels = []
    
    print("ä¸ºæ ‡æ³¨å¯¹æ„å»ºè”åˆç‰¹å¾...")
    for _, row in tqdm(labeled_data.iterrows(), total=len(labeled_data)):
        # en_idå¯¹åº”é«˜å¾·ï¼Œzh_idå¯¹åº”ç™¾åº¦
        en_id = row['en_id']  # é«˜å¾·ID
        zh_id = row['zh_id']  # ç™¾åº¦ID
        label = row['label']
        
        # è·å–ä¸¤ä¸ªPOIçš„å¢å¼ºç‰¹å¾
        en_feat = enhanced_feat_en.get(en_id)  # é«˜å¾·ç‰¹å¾
        zh_feat = enhanced_feat_zh.get(zh_id)  # ç™¾åº¦ç‰¹å¾
        
        if en_feat is None or zh_feat is None:
            continue
        
        # æ‹¼æ¥ä¸¤ä¸ªPOIçš„ç‰¹å¾ä½œä¸ºåˆ†ç±»å™¨è¾“å…¥
        combined_feature = np.concatenate([en_feat, zh_feat])
        features.append(combined_feature)
        labels.append(label)
    
    features = np.array(features)
    labels = np.array(labels)
    
    print(f"ç‰¹å¾ç»´åº¦: {features.shape}")
    print(f"æ­£æ ·æœ¬æ•°: {sum(labels)}, è´Ÿæ ·æœ¬æ•°: {len(labels) - sum(labels)}")
    
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        features, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    # è®­ç»ƒLightGBM
    model = lgb.LGBMClassifier(
        objective='binary',
        metric='binary_logloss',
        random_state=42,
        n_estimators=1000,
        learning_rate=0.05,
        num_leaves=31,
        verbosity=1
    )
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)]
    )
    
    # è¯„ä¼°æ¨¡å‹
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    report = classification_report(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(report)
    print(f"F1åˆ†æ•°: {f1:.4f}")
    print(f"AUCåˆ†æ•°: {auc:.4f}")
    
    return model

# --- ä¸»æµç¨‹ ---
# --- ä¸»æµç¨‹ ---
def main():
    print("====== å¼€å§‹æ‰§è¡ŒPOIåŒ¹é…æ¨¡å‹è®­ç»ƒ ======")
    
    if DEBUG_MODE:
        print(f"ğŸ› DEBUGæ¨¡å¼å·²å¯ç”¨ - åªå¤„ç†å‰ {DEBUG_DATA_LIMIT} æ¡æ•°æ®")
        print("   è¿™å°†å¤§å¤§åŠ å¿«å¤„ç†é€Ÿåº¦ï¼Œé€‚åˆæµ‹è¯•å’Œè°ƒè¯•")
    
    # è®¾ç½®è®¾å¤‡
    device = DEVICE if torch.cuda.is_available() or DEVICE == 'cpu' else 'cpu'
    print(f"ä½¿ç”¨çš„è®¾å¤‡: {device}")
    print(f"LLMæ¨¡å‹ç±»å‹: {LLM_MODEL_TYPE}")
    print(f"LLMæ¨¡å‹åç§°: {LLM_MODEL_NAME}")
    
    # åŠ è½½æ–‡æœ¬ç¼–ç æ¨¡å‹
    text_model = SentenceTransformer(MODEL_PATH, device=device)
    
    # åˆå§‹åŒ–Geohashç¼–ç å™¨
    base32_chars = "0123456789bcdefghjkmnpqrstuvwxyz"
    charset_size = len(base32_chars)
    geohash_encoder = GeohashEncoder(
        charset_size=charset_size,
        embedding_dim=8,      # å­—ç¬¦åµŒå…¥ç»´åº¦
        hidden_dim=16,       # GRUéšè—å±‚ç»´åº¦
        output_dim=32,       # è¾“å‡ºç»´åº¦
        device=device
    )
    geohash_encoder.eval()   # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    
    # åˆå§‹åŒ–LLMæ–‡æœ¬å¢å¼ºå™¨ï¼ˆå¯é€‰ï¼‰
    llm_enhancer = None
    
    try:
        print("\nåˆå§‹åŒ–LLMæ–‡æœ¬å¢å¼ºå™¨...")
        
        if LLM_MODEL_TYPE == "ollama":
            # ä½¿ç”¨OLLAMAæ¨¡å‹
            print(f"ä½¿ç”¨OLLAMAæ¨¡å‹: {LLM_MODEL_NAME}")
            llm_enhancer = LLMTextEnhancer(
                llm_model_path=LLM_MODEL_NAME,
                text_encoder_name="paraphrase-multilingual-MiniLM-L12-v2",
                use_llm=True,
                device=device,
                target_dim=384,
                model_type="ollama",
                ollama_base_url=OLLAMA_BASE_URL
            )
        else:
            # ä½¿ç”¨HuggingFaceæ¨¡å‹
            if LLM_MODEL_NAME.startswith("microsoft/") or LLM_MODEL_NAME.startswith("gpt"):
                # åœ¨çº¿æ¨¡å‹
                model_path = LLM_MODEL_NAME
            elif os.path.isabs(LLM_MODEL_NAME):
                # ç»å¯¹è·¯å¾„
                model_path = LLM_MODEL_NAME
            else:
                # ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æ‹¼æ¥
                if LLM_MODEL_NAME.startswith("models/"):
                    # å¦‚æœå·²ç»åŒ…å«models/å‰ç¼€ï¼Œç›´æ¥ä½¿ç”¨PROJECT_ROOTæ‹¼æ¥
                    model_path = os.path.join(PROJECT_ROOT, LLM_MODEL_NAME)
                else:
                    # å¦åˆ™ä½¿ç”¨MODEL_DIRæ‹¼æ¥
                    model_path = os.path.join(MODEL_DIR, LLM_MODEL_NAME)
            
            print(f"ä½¿ç”¨HuggingFaceæ¨¡å‹: {model_path}")
            llm_enhancer = LLMTextEnhancer(
                llm_model_path=model_path,
                text_encoder_name="paraphrase-multilingual-MiniLM-L12-v2",
                use_llm=True,
                device=device,
                target_dim=384,
                model_type="huggingface"
            )
        
        print("LLMæ–‡æœ¬å¢å¼ºå™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"LLMæ–‡æœ¬å¢å¼ºå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("å°†ä½¿ç”¨åŸå§‹ç‰¹å¾ç»§ç»­è®­ç»ƒ...")
    
    # --- å¤„ç†ç™¾åº¦ä¸­æ–‡æ•°æ® (zhå¯¹åº”ç™¾åº¦) ---
    print("\nå¤„ç†ç™¾åº¦ä¸­æ–‡POIæ•°æ®...")
    
    # å°è¯•åŠ è½½ç¼“å­˜çš„æ–‡æœ¬åµŒå…¥
    cache_suffix = f"_debug_{DEBUG_DATA_LIMIT}" if DEBUG_MODE else ""
    zh_cache_path = os.path.join(CACHE_DIR, f'text_emb_zh{cache_suffix}.pkl')
    if os.path.exists(zh_cache_path):
        print(f"åŠ è½½ç¼“å­˜çš„ç™¾åº¦æ–‡æœ¬åµŒå…¥{' (DEBUGæ¨¡å¼)' if DEBUG_MODE else ''}...")
        with open(zh_cache_path, 'rb') as f:
            cache_data = pickle.load(f)
            df_zh, text_emb_zh = cache_data['df'], cache_data['embeddings']
    else:
        df_zh, text_emb_zh = generate_text_embeddings(BD_CHINESE_POI_PATH, text_model, device)
        # ä¿å­˜åˆ°ç¼“å­˜
        print(f"ä¿å­˜ç™¾åº¦æ–‡æœ¬åµŒå…¥åˆ°ç¼“å­˜{' (DEBUGæ¨¡å¼)' if DEBUG_MODE else ''}...")
        with open(zh_cache_path, 'wb') as f:
            pickle.dump({'df': df_zh, 'embeddings': text_emb_zh}, f)
    
    df_zh = add_geohash(df_zh)  # ä½¿ç”¨ add_geohash å‡½æ•°æ·»åŠ  geohash åˆ—
    
    # å°è¯•åŠ è½½ç¼“å­˜çš„GATåµŒå…¥
    zh_gat_cache_path = os.path.join(CACHE_DIR, f'gat_emb_zh{cache_suffix}.pkl')
    if os.path.exists(zh_gat_cache_path):
        print(f"åŠ è½½ç¼“å­˜çš„ç™¾åº¦GATåµŒå…¥{' (DEBUGæ¨¡å¼)' if DEBUG_MODE else ''}...")
        with open(zh_gat_cache_path, 'rb') as f:
            gat_emb_zh = pickle.load(f)
    else:
        gat_emb_zh = generate_gat_embeddings(df_zh, device)
        # ä¿å­˜åˆ°ç¼“å­˜
        print(f"ä¿å­˜ç™¾åº¦GATåµŒå…¥åˆ°ç¼“å­˜{' (DEBUGæ¨¡å¼)' if DEBUG_MODE else ''}...")
        with open(zh_gat_cache_path, 'wb') as f:
            pickle.dump(gat_emb_zh, f)
    
    # --- å¤„ç†é«˜å¾·ä¸­æ–‡æ•°æ® (enå¯¹åº”é«˜å¾·) ---
    print("\nå¤„ç†é«˜å¾·ä¸­æ–‡POIæ•°æ®...")
    
    # å°è¯•åŠ è½½ç¼“å­˜çš„æ–‡æœ¬åµŒå…¥
    en_cache_path = os.path.join(CACHE_DIR, f'text_emb_en{cache_suffix}.pkl')
    if os.path.exists(en_cache_path):
        print(f"åŠ è½½ç¼“å­˜çš„é«˜å¾·æ–‡æœ¬åµŒå…¥{' (DEBUGæ¨¡å¼)' if DEBUG_MODE else ''}...")
        with open(en_cache_path, 'rb') as f:
            cache_data = pickle.load(f)
            df_en, text_emb_en = cache_data['df'], cache_data['embeddings']
    else:
        df_en, text_emb_en = generate_text_embeddings(GD_CHINESE_POI_PATH, text_model, device)
        # ä¿å­˜åˆ°ç¼“å­˜
        print(f"ä¿å­˜é«˜å¾·æ–‡æœ¬åµŒå…¥åˆ°ç¼“å­˜{' (DEBUGæ¨¡å¼)' if DEBUG_MODE else ''}...")
        with open(en_cache_path, 'wb') as f:
            pickle.dump({'df': df_en, 'embeddings': text_emb_en}, f)
    
    df_en = add_geohash(df_en)  # ä½¿ç”¨ add_geohash å‡½æ•°æ·»åŠ  geohash åˆ—
    
    # å°è¯•åŠ è½½ç¼“å­˜çš„GATåµŒå…¥
    en_gat_cache_path = os.path.join(CACHE_DIR, f'gat_emb_en{cache_suffix}.pkl')
    if os.path.exists(en_gat_cache_path):
        print(f"åŠ è½½ç¼“å­˜çš„é«˜å¾·GATåµŒå…¥{' (DEBUGæ¨¡å¼)' if DEBUG_MODE else ''}...")
        with open(en_gat_cache_path, 'rb') as f:
            gat_emb_en = pickle.load(f)
    else:
        gat_emb_en = generate_gat_embeddings(df_en, device)
        # ä¿å­˜åˆ°ç¼“å­˜
        print(f"ä¿å­˜é«˜å¾·GATåµŒå…¥åˆ°ç¼“å­˜{' (DEBUGæ¨¡å¼)' if DEBUG_MODE else ''}...")
        with open(en_gat_cache_path, 'wb') as f:
            pickle.dump(gat_emb_en, f)

    
    # --- åˆ›å»ºå¢å¼ºç‰¹å¾ ---
    print("\nåˆ›å»ºå¢å¼ºç‰¹å¾...")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹å’Œåç§°ç”Ÿæˆç¼“å­˜æ–‡ä»¶å
    model_cache_suffix = f"_{LLM_MODEL_TYPE}_{LLM_MODEL_NAME.replace('/', '_').replace(':', '_')}"
    enhanced_features_cache_path = os.path.join(CACHE_DIR, f'enhanced_features{cache_suffix}{model_cache_suffix}.pkl')
    
    # å°è¯•åŠ è½½ç¼“å­˜çš„å¢å¼ºç‰¹å¾
    if os.path.exists(enhanced_features_cache_path):
        print(f"åŠ è½½ç¼“å­˜çš„å¢å¼ºç‰¹å¾ (æ¨¡å‹: {LLM_MODEL_TYPE}/{LLM_MODEL_NAME}){' (DEBUGæ¨¡å¼)' if DEBUG_MODE else ''}...")
        with open(enhanced_features_cache_path, 'rb') as f:
            cache_data = pickle.load(f)
            enhanced_features_zh = cache_data['zh']
            enhanced_features_en = cache_data['en']
        print(f"ä»ç¼“å­˜åŠ è½½äº† {len(enhanced_features_zh)} ä¸ªç™¾åº¦ç‰¹å¾å’Œ {len(enhanced_features_en)} ä¸ªé«˜å¾·ç‰¹å¾")
    else:
        enhanced_features_zh = create_poi_enhanced_features(
            df_zh, text_emb_zh, gat_emb_zh, geohash_encoder, device, llm_enhancer
        )
        enhanced_features_en = create_poi_enhanced_features(
            df_en, text_emb_en, gat_emb_en, geohash_encoder, device, llm_enhancer
        )
        
        # ä¿å­˜å¢å¼ºç‰¹å¾åˆ°ç¼“å­˜
        print(f"ä¿å­˜å¢å¼ºç‰¹å¾åˆ°ç¼“å­˜ (æ¨¡å‹: {LLM_MODEL_TYPE}/{LLM_MODEL_NAME}){' (DEBUGæ¨¡å¼)' if DEBUG_MODE else ''}...")
        with open(enhanced_features_cache_path, 'wb') as f:
            pickle.dump({
                'zh': enhanced_features_zh,
                'en': enhanced_features_en,
                'model_type': LLM_MODEL_TYPE,
                'model_name': LLM_MODEL_NAME,
                'timestamp': datetime.now().isoformat()
            }, f)
        print(f"å·²ç¼“å­˜ {len(enhanced_features_zh)} ä¸ªç™¾åº¦ç‰¹å¾å’Œ {len(enhanced_features_en)} ä¸ªé«˜å¾·ç‰¹å¾")
    
    # --- ä½¿ç”¨æ ‡æ³¨æ•°æ®è®­ç»ƒåˆ†ç±»å™¨ ---
    classifier = train_classifier_with_enhanced_features(
        enhanced_features_en, enhanced_features_zh, LABELED_DATA_PATH
    )
    
    print("\n====== æ¨¡å‹è®­ç»ƒå®Œæˆ! ======")

if __name__ == '__main__':
    main()